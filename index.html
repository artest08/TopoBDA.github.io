<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TopoBDA: Bezier Deformable Attention for Road Topology</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Header Section -->
    <header class="hero">
        <div class="container">
            <div class="hero-content">
                <h1 class="hero-title">TopoBDA</h1>
                <p class="hero-subtitle">Towards Bezier Deformable Attention for Road Topology Understanding</p>
                <div class="authors">
                    <span class="author">Muhammet Esat Kalfaoglu*</span>
                    <span class="author">Halil Ibrahim Ozturk</span>
                    <span class="author">Ozsel Kilinc</span>
                    <span class="author">Alptekin Temizel</span>
                </div>
                <div class="affiliations">
                    <span class="affiliation">Middle East Technical University ‚Ä¢ Togg/Trutek AI Team</span>
                </div>
                <div class="action-buttons">
                    <a href="https://arxiv.org/abs/2412.18951" class="btn btn-primary" target="_blank">
                        üìÑ Paper
                    </a>
                    <a href="#citation" class="btn btn-secondary">
                        üìã Cite
                    </a>
                </div>
            </div>
        </div>
    </header>

    <!-- Abstract -->
    <section class="section" id="abstract">
        <div class="container">
            <h2>Abstract</h2>
            <div class="abstract-content">
                <p>
                    Understanding road topology is crucial for autonomous driving. This paper introduces <strong>TopoBDA</strong> 
                    (Topology with Bezier Deformable Attention), a novel approach that enhances road topology comprehension 
                    by leveraging Bezier Deformable Attention (BDA). TopoBDA processes multi-camera 360-degree imagery to 
                    generate Bird's Eye View (BEV) features, which are refined through a transformer decoder employing BDA. 
                    BDA utilizes Bezier control points to drive the deformable attention mechanism, improving the detection 
                    and representation of elongated and thin polyline structures, such as lane centerlines.
                </p>
                <p>
                    Additionally, TopoBDA integrates two auxiliary components: an instance mask formulation loss and a 
                    one-to-many set prediction loss strategy, to further refine centerline detection and enhance road 
                    topology understanding. Experimental evaluations on the OpenLane-V2 dataset demonstrate that TopoBDA 
                    outperforms existing methods, achieving state-of-the-art results in centerline detection and topology 
                    reasoning. TopoBDA also achieves the best results on the OpenLane-V1 dataset in 3D lane detection. 
                    Further experiments on integrating multi-modal data‚Äîsuch as LiDAR, radar, and SDMap‚Äîshow that multimodal 
                    inputs can further enhance performance in road topology understanding.
                </p>
            </div>
        </div>
    </section>

    <!-- Research Highlights -->
    <section class="section" id="highlights-research">
        <div class="container">
            <h2>Research Highlights</h2>
            <div class="highlights-research-grid">
                <div class="highlight-item">
                    <div class="highlight-icon">üîó</div>
                    <h3>Novel MPDA Integration to Bezier Structures</h3>
                    <p>First-time integration of Multi-Point Deformable Attention (MPDA) into Bezier keypoint-dependent transformer decoders, enhancing centerline detection for elongated polyline structures.</p>
                </div>
                <div class="highlight-item">
                    <div class="highlight-icon">üéØ</div>
                    <h3>Bezier Deformable Attention (BDA)</h3>
                    <p>Novel attention mechanism utilizing Bezier control points as reference points, achieving superior performance with reduced computational complexity compared to traditional MPDA approaches.</p>
                </div>
                <div class="highlight-item">
                    <div class="highlight-icon">üîß</div>
                    <h3>Instance Mask Formulation</h3>
                    <p>Indirect auxiliary supervision through instance mask prediction and Mask-L1 mix matcher, improving centerline detection without inference overhead.</p>
                </div>
                <div class="highlight-item">
                    <div class="highlight-icon">üåê</div>
                    <h3>Multi-modal Fusion Analysis</h3>
                    <p>First comprehensive evaluation of camera, LiDAR, radar, and SDMap integration for road topology understanding, achieving state-of-the-art multi-modal performance.</p>
                </div>
                <div class="highlight-item">
                    <div class="highlight-icon">‚ö°</div>
                    <h3>Comprehensive Attention Mechanism Analysis</h3>
                    <p>Systematic comparative analysis of attention mechanisms for road topology understanding, evaluating computational complexity, runtime efficiency, and performance across Standard, Masked, Single-Point, Multi-Point, and Bezier Deformable Attention with detailed FLOPS and parameter analysis.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Architecture Overview -->
    <section class="section" id="architecture">
        <div class="container">
            <h2>Method Overview</h2>
            <div class="figure-container">
                <div class="image-container">
                    <img src="images/topoBDA.png" alt="TopoBDA Architecture - Multi-camera BEV feature extraction with Bezier Deformable Attention" class="research-figure architecture-figure" loading="lazy">
                </div>
                <div class="figure-caption">
                    <strong>Figure 1: TopoBDA Architecture.</strong> Overview of the TopoBDA architecture. The TopoBDA architecture is based on the instance query concept. The extracted BEV features from the multiple camera images are fed into the transformer decoder. The decoder outputs Bezier control points for each query, which are then converted into centerline instances via matrix multiplication. Additionally, each centerline query predicts instance masks, but only during training.
                </div>
            </div>
        </div>
    </section>

    <!-- Key Innovation: Bezier Deformable Attention -->
    <section class="section" id="innovation">
        <div class="container">
            <h2>Key Innovation: Bezier Deformable Attention</h2>
            <div class="innovation-grid">
                <div class="innovation-item">
                    <h3>Attention Evolution</h3>
                    <div class="figure-container">
                        <div class="image-container">
                            <img src="images/spda_vs_mpda.png" alt="Attention Mechanism Evolution: SPDA to MPDA to BDA comparison" class="research-figure comparison-figure" loading="lazy">
                        </div>
                        <div class="figure-caption">
                            <strong>Figure 2: Attention Mechanism Evolution.</strong> Comparison of Single-Point Deformable Attention (SPDA) with Multi-Point (MPDA) and Bezier (BDA) Deformable Attention. MPDA and BDA share the same underlying mechanism but differ in the selection of multiple reference points (p_x,p_y).
                        </div>
                    </div>
                </div>
                <div class="innovation-item">
                    <h3>Implementation Efficiency</h3>
                    <div class="figure-container">
                        <div class="image-container">
                            <img src="images/msda_vs_bda.png" alt="BDA vs MPDA Implementation Efficiency Comparison" class="research-figure comparison-figure" loading="lazy">
                        </div>
                        <div class="figure-caption">
                            <strong>Figure 3: BDA vs MPDA Implementation.</strong> Comparison of Multi-Point Deformable Attention (MPDA) and Bezier Deformable Attention (BDA): MPDA necessitates an additional matrix multiplication block within each transformer decoder. Despite their different input utilizations as reference points, the mechanisms of MPDA and BDA blocks are fundamentally the same.
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Methodology Section -->
    <section class="section" id="methodology">
        <div class="container">
            <h2>Methodology</h2>
            
            <div class="methodology-grid">
                <div class="method-card">
                    <h3>üèóÔ∏è Instance Query Architecture</h3>
                    <p>Multi-camera 360-degree imagery processed through BEV feature extraction, refined via transformer decoder with sparse query approach where each query represents a centerline instance rather than individual points.</p>
                </div>
                
                <div class="method-card">
                    <h3>üìê Bezier Control Point Regression</h3>
                    <p>Compact polyline representation through Bezier control points, enabling efficient centerline modeling via matrix multiplication operations while reducing computational complexity at regression heads.</p>
                </div>
                
                <div class="method-card">
                    <h3>üéØ Auxiliary Training Components</h3>
                    <p>Instance mask prediction and Mask-L1 mix matcher during training enhance centerline detection accuracy. One-to-many set prediction loss improves training convergence without inference overhead.</p>
                </div>
            </div>
            
            <!-- BDA Layers Visualization -->
            <div class="figure-container">
                <div class="image-container">
                    <img src="images/BDA_layers.png" alt="TopoBDA Layers with Bezier Deformable Attention - Iterative refinement across decoder layers" class="research-figure layers-figure" loading="lazy">
                </div>
                <div class="figure-caption">
                    <strong>Figure 6: BDA Layers Visualization.</strong> This figure visualizes the layers of TopoBDA, each driven by Bezier Deformable Attention (BDA) using control points predicted through iterative refinement. Note that iterative refinement is not applicable to the first layer, which uses direct prediction.
                </div>
            </div>
        </div>
    </section>

    <!-- Experimental Results -->
    <section class="section" id="results">
        <div class="container">
            <h2>Experimental Results</h2>
            
            <!-- State-of-the-art Comparison -->
            <div class="results-container">
                <h3>üèÜ State-of-the-Art Performance on OpenLane-V2 (Subset-A)</h3>
                <div class="table-responsive">
                    <table class="performance-table">
                        <thead>
                            <tr>
                                <th>Method</th>
                                <th>Sensor</th>
                                <th>DET<sub>l</sub></th>
                                <th>DET<sub>t</sub></th>
                                <th>TOP<sub>ll</sub></th>
                                <th>TOP<sub>lt</sub></th>
                                <th>OLS</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>TopoNet</td>
                                <td>C</td>
                                <td>28.6</td>
                                <td>48.6</td>
                                <td>10.9</td>
                                <td>23.9</td>
                                <td>39.8</td>
                            </tr>
                            <tr>
                                <td>TopoMLP</td>
                                <td>C</td>
                                <td>28.5</td>
                                <td>49.5</td>
                                <td>21.7</td>
                                <td>26.9</td>
                                <td>44.1</td>
                            </tr>
                            <tr>
                                <td>TopoFormer</td>
                                <td>C</td>
                                <td>34.7</td>
                                <td>48.2</td>
                                <td>24.1</td>
                                <td>29.5</td>
                                <td>46.3</td>
                            </tr>
                            <tr>
                                <td>TopoMaskV2</td>
                                <td>C</td>
                                <td>34.5</td>
                                <td>53.8</td>
                                <td>24.5</td>
                                <td>35.6</td>
                                <td>49.4</td>
                            </tr>
                            <tr class="highlight">
                                <td><strong>TopoBDA (Ours)</strong></td>
                                <td><strong>C</strong></td>
                                <td><strong>38.9</strong></td>
                                <td><strong>54.3</strong></td>
                                <td><strong>27.6</strong></td>
                                <td><strong>37.3</strong></td>
                                <td><strong>51.7</strong></td>
                            </tr>
                            <tr class="highlight">
                                <td><strong>TopoBDA (Ours)</strong></td>
                                <td><strong>C + L</strong></td>
                                <td><strong>47.3</strong></td>
                                <td><strong>54.0</strong></td>
                                <td><strong>35.5</strong></td>
                                <td><strong>41.9</strong></td>
                                <td><strong>56.4</strong></td>
                            </tr>
                            <tr class="highlight">
                                <td><strong>TopoBDA (Ours)</strong></td>
                                <td><strong>C + SD</strong></td>
                                <td><strong>42.7</strong></td>
                                <td><strong>52.4</strong></td>
                                <td><strong>34.3</strong></td>
                                <td><strong>41.7</strong></td>
                                <td><strong>54.6</strong></td>
                            </tr>
                            <tr class="highlight">
                                <td><strong>TopoBDA (Ours)</strong></td>
                                <td><strong>C + L + SD</strong></td>
                                <td><strong>52.0</strong></td>
                                <td><strong>52.4</strong></td>
                                <td><strong>38.5</strong></td>
                                <td><strong>45.3</strong></td>
                                <td><strong>58.4</strong></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
                <h3>üèÜ State-of-the-Art Performance on OpenLane-V1 (3D Lane Detection)</h3>
                <div class="table-responsive">
                    <table class="performance-table">
                        <thead>
                            <tr>
                                <th>Distance</th>
                                <th>Method</th>
                                <th>Backbone</th>
                                <th>F1-Score ‚Üë</th>
                                <th>X-error near (m) ‚Üì</th>
                                <th>X-error far (m) ‚Üì</th>
                                <th>Z-error near (m) ‚Üì</th>
                                <th>Z-error far (m) ‚Üì</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr style="border-bottom: 2px solid #ddd;">
                                <td rowspan="7" style="vertical-align: middle; font-weight: bold;">1.5m</td>
                                <td>PersFormer</td>
                                <td>ResNet-50</td>
                                <td>52.7</td>
                                <td>0.307</td>
                                <td>0.319</td>
                                <td>0.083</td>
                                <td>0.117</td>
                            </tr>
                            <tr>
                                <td>Anchor3DLane</td>
                                <td>ResNet-50</td>
                                <td>57.5</td>
                                <td>0.229</td>
                                <td><strong>0.243</strong></td>
                                <td>0.079</td>
                                <td>0.106</td>
                            </tr>
                            <tr>
                                <td>GroupLane</td>
                                <td>ResNet-50</td>
                                <td>60.2</td>
                                <td>0.371</td>
                                <td>0.476</td>
                                <td>0.220</td>
                                <td>0.357</td>
                            </tr>
                            <tr>
                                <td>LaneCPP</td>
                                <td>EffNet-B7</td>
                                <td>60.3</td>
                                <td>0.264</td>
                                <td>0.310</td>
                                <td>0.077</td>
                                <td>0.117</td>
                            </tr>
                            <tr>
                                <td>LATR</td>
                                <td>ResNet-50</td>
                                <td>61.9</td>
                                <td><strong>0.219</strong></td>
                                <td style="text-decoration: underline;">0.259</td>
                                <td style="text-decoration: underline;">0.075</td>
                                <td style="text-decoration: underline;">0.104</td>
                            </tr>
                            <tr>
                                <td>PVALane</td>
                                <td>ResNet-50</td>
                                <td style="text-decoration: underline;">62.7</td>
                                <td>0.232</td>
                                <td style="text-decoration: underline;">0.259</td>
                                <td>0.092</td>
                                <td>0.118</td>
                            </tr>
                            <tr class="highlight" style="border-bottom: 2px solid #ddd;">
                                <td><strong>TopoBDA (Ours)</strong></td>
                                <td><strong>ResNet-50</strong></td>
                                <td><strong>63.9</strong></td>
                                <td style="text-decoration: underline;"><strong>0.224</strong></td>
                                <td><strong>0.243</strong></td>
                                <td><strong>0.069</strong></td>
                                <td><strong>0.101</strong></td>
                            </tr>
                            <tr>
                                <td rowspan="4" style="vertical-align: middle; font-weight: bold;">0.5m</td>
                                <td>PersFormer</td>
                                <td>ResNet-50</td>
                                <td>43.2</td>
                                <td>0.229</td>
                                <td>0.245</td>
                                <td>0.078</td>
                                <td>0.106</td>
                            </tr>
                            <tr>
                                <td>DV-3DLane</td>
                                <td>ResNet-34</td>
                                <td>52.9</td>
                                <td>0.173</td>
                                <td>0.212</td>
                                <td style="text-decoration: underline;">0.069</td>
                                <td style="text-decoration: underline;">0.098</td>
                            </tr>
                            <tr>
                                <td>LATR</td>
                                <td>ResNet-50</td>
                                <td style="text-decoration: underline;">54.0</td>
                                <td style="text-decoration: underline;">0.171</td>
                                <td style="text-decoration: underline;">0.201</td>
                                <td>0.072</td>
                                <td>0.099</td>
                            </tr>
                            <tr class="highlight">
                                <td><strong>TopoBDA (Ours)</strong></td>
                                <td><strong>ResNet-50</strong></td>
                                <td><strong>57.9</strong></td>
                                <td><strong>0.157</strong></td>
                                <td><strong>0.179</strong></td>
                                <td><strong>0.067</strong></td>
                                <td><strong>0.087</strong></td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h3>Instance Mask Formulation Ablation</h3>
                <div class="table-responsive">
                    <table class="performance-table">
                        <thead>
                            <tr>
                                <th>IMAL</th>
                                <th>ML1M</th>
                                <th>DET<sub>l</sub></th>
                                <th>DET<sub>l_ch</sub></th>
                                <th>TOP<sub>ll</sub></th>
                                <th>OLS<sub>l</sub></th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>‚úó</td>
                                <td>‚úó</td>
                                <td>37.0</td>
                                <td>39.8</td>
                                <td>29.0</td>
                                <td>43.6</td>
                            </tr>
                            <tr>
                                <td>‚úì</td>
                                <td>‚úó</td>
                                <td style="text-decoration: underline;">40.7</td>
                                <td style="text-decoration: underline;">42.1</td>
                                <td style="text-decoration: underline;">32.4</td>
                                <td style="text-decoration: underline;">46.6</td>
                            </tr>
                            <tr class="highlight">
                                <td><strong>‚úì</strong></td>
                                <td><strong>‚úì</strong></td>
                                <td><strong>40.8</strong></td>
                                <td><strong>45.8</strong></td>
                                <td><strong>32.9</strong></td>
                                <td><strong>48.0</strong></td>
                            </tr>
                        </tbody>
                    </table>
                    <p class="table-caption">
                        <strong>IMAL</strong>: Instance Mask Auxiliary Loss | <strong>ML1M</strong>: Mask-L1 Mix Matcher<br>
                        Showing significant improvements from instance mask formulation and mask-L1 mix matcher: +3.8 points in DET<sub>l</sub> and +4.4 points in OLS<sub>l</sub>.
                    </p>
                </div>

                <h3>Attention Mechanism Ablation</h3>
                <div class="table-responsive">
                    <table class="performance-table">
                        <thead>
                            <tr>
                                <th>Attention Type</th>
                                <th>DET<sub>l</sub></th>
                                <th>DET<sub>l_ch</sub></th>
                                <th>TOP<sub>ll</sub></th>
                                <th>OLS<sub>l</sub></th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>SA (Standard Attention)</td>
                                <td>34.5</td>
                                <td>38.4</td>
                                <td>25.1</td>
                                <td>41.0</td>
                            </tr>
                            <tr>
                                <td>MA (Masked Attention)</td>
                                <td>35.8</td>
                                <td>40.2</td>
                                <td>26.9</td>
                                <td>42.6</td>
                            </tr>
                            <tr>
                                <td>SPDA (Single-Point Deformable Attention)</td>
                                <td>38.3</td>
                                <td>39.8</td>
                                <td>29.5</td>
                                <td>44.1</td>
                            </tr>
                            <tr>
                                <td>MPDA4 (Multi-Point Deformable Attention)</td>
                                <td>40.2</td>
                                <td>45.0</td>
                                <td>32.6</td>
                                <td>47.4</td>
                            </tr>
                            <tr>
                                <td>MPDA16 (16-Point Deformable Attention)</td>
                                <td style="text-decoration: underline;">40.3</td>
                                <td style="text-decoration: underline;">45.1</td>
                                <td style="text-decoration: underline;">32.7</td>
                                <td style="text-decoration: underline;">47.5</td>
                            </tr>
                            <tr class="highlight">
                                <td><strong>BDA (Bezier Deformable Attention)</strong></td>
                                <td><strong>40.8</strong></td>
                                <td><strong>45.8</strong></td>
                                <td><strong>32.9</strong></td>
                                <td><strong>48.0</strong></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </section>

    <!-- Visual Results -->
    <section class="section" id="visual-results">
        <div class="container">
            <h2>Visual Results</h2>
            
            <!-- Closed-loop and Fusion Analysis -->
            <div class="figure-container">
                <div class="image-container">
                    <img src="images/clsd_fuse_analysis.png" alt="TopoBDA Closed-loop Analysis and Multi-modal Fusion Results" class="research-figure analysis-figure" loading="lazy">
                </div>
                <div class="figure-caption">
                    <strong>Figure 5: Multi-modal Fusion Analysis.</strong> Visual demonstration in the BEV domain showing the impact of lidar and SDMap additions in Subset-A of the OpenLane-V2 dataset. C, SD, and L represent the camera, SDMap, and lidar, respectively. Green polylines indicate the ground truth, and red polylines represent predictions. The circular regions highlight the inaccurate regions compared to other reference BEV images.
                </div>
            </div>

            <!-- Dataset Overview -->
            <div class="figure-container">
                <div class="image-container">
                    <img src="images/dataset.png" alt="OpenLane-V2 Dataset Overview - Perspective and Bird's Eye View samples with annotations" class="research-figure dataset-figure" loading="lazy">
                </div>
                <div class="figure-caption">
                    <strong>Figure 7: Dataset Ground Truth Visualization.</strong> Perspective-view (PV) and bird's-eye-view (BEV) samples from the OpenLane-V2 dataset. (a) and (d) show centerline instances in PV and BEV domains, respectively, with each color representing a distinct instance. (b) and (c) illustrate centerlines with colors indicating topological relationships between centerlines and traffic elements in PV and BEV. (e) visualizes the topological relationships among different centerlines, where directed arrows indicate connectivity between centerlines.
                </div>
            </div>

            <!-- Video Demonstrations -->
            <div class="video-section">
                <h3>üé• TopoBDA Video Demonstrations</h3>
                
                <!-- Front View Video -->
                <div class="video-container">
                    <h4 style="padding: 1rem; margin: 0; background: #f8f9fa; border-bottom: 1px solid #dee2e6;">
                        üìπ Front View Perspective - Ground Truth vs Predictions
                    </h4>
                    <video controls style="width: 100%; height: auto; display: block; background: #000;">
                        <source src="visualization_videos/front_view_web_hq.mp4" type="video/mp4">
                        <p style="padding: 2rem; text-align: center;">
                            Your browser does not support video playback.
                        </p>
                    </video>
                    <div style="padding: 1rem; background: #f8f9fa; font-size: 0.9rem; line-height: 1.6;">
                        <strong>Front View Analysis:</strong> Demonstrates TopoBDA's centerline detection performance in the front camera perspective view, comparing ground truth and model predictions. Centerlines are shown in blue colors, while topological relationships between centerlines and traffic elements are indicated by arrows. The arrow colors represent different traffic element types, highlighting the spatial connectivity and semantic understanding capabilities of the model.
                    </div>
                </div>

                <!-- Multi-View Video -->
                <div class="video-container">
                    <h4 style="padding: 1rem; margin: 0; background: #f8f9fa; border-bottom: 1px solid #dee2e6;">
                        üìπ Multi-View 360¬∞ Coverage - Centerline Instance Predictions
                    </h4>
                    <video controls style="width: 100%; height: auto; display: block; background: #000;">
                        <source src="visualization_videos/multi_view_web_hq.mp4" type="video/mp4">
                        <p style="padding: 2rem; text-align: center;">
                            Your browser does not support video playback.
                        </p>
                    </video>
                    <div style="padding: 1rem; background: #f8f9fa; font-size: 0.9rem; line-height: 1.6;">
                        <strong>Multi-View Analysis:</strong> Showcases TopoBDA's performance across all camera views with comprehensive 360-degree coverage. The visualization focuses on centerline instance predictions, where different instances are distinguished by distinct colors. This demonstration highlights the model's capability to accurately detect and differentiate multiple centerline instances across various viewing perspectives.
                    </div>
                </div>

                <!-- BEV All Modalities Video -->
                <div class="video-container">
                    <h4 style="padding: 1rem; margin: 0; background: #f8f9fa; border-bottom: 1px solid #dee2e6;">
                        üìπ Bird's Eye View Analysis - Multi-Modal Fusion
                    </h4>
                    <video controls style="width: 100%; height: auto; display: block; background: #000;">
                        <source src="visualization_videos/bev_all_web_hq.mp4" type="video/mp4">
                        <p style="padding: 2rem; text-align: center;">
                            Your browser does not support video playback.
                        </p>
                    </video>
                    <div style="padding: 1rem; background: #f8f9fa; font-size: 0.9rem; line-height: 1.6;">
                        <strong>BEV Analysis:</strong> Comprehensive Bird's Eye View visualization showcasing nine distinct perspectives with detailed ground truth-prediction correspondences: (1) Ground truth traffic element-centerline topology with highlighted relationships, (2) Ground truth instances with color-coded differentiation, (3) Predicted traffic element-centerline topology with highlighted relationships (correspondence to 1), (4) Predicted instances with color-coded differentiation (correspondence to 2), (5) Ground truth centerline-to-centerline topology relationships, (6) Predicted centerline-to-centerline topology relationships (correspondence to 5), (7) Overlaid prediction comparison analysis, (8) SDMap input visualization, and (9) LiDAR point cloud input visualization. This sequence demonstrates TopoBDA's multi-modal fusion capabilities integrating camera, SDMap, and LiDAR data for robust road topology understanding.
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Conclusion -->
    <section class="section" id="conclusion">
        <div class="container">
            <h2>Conclusion</h2>
            <div class="conclusion-content">
                <p>
                    Experimental evaluations demonstrate that <strong>TopoBDA achieves state-of-the-art performance</strong> 
                    across both subsets of the OpenLane-V2 dataset. Specifically, TopoBDA surpasses existing methods with a 
                    <strong>DET<sub>l</sub> score of 38.9</strong> and an <strong>OLS score of 51.7</strong> in Subset-A, 
                    and a <strong>DET<sub>l</sub> score of 45.1</strong> and an <strong>OLS score of 54.3</strong> in Subset-B.
                </p>
                <p>
                    The integration of multi-modal data significantly boosts performance: fusing camera and LiDAR data increases 
                    the OLS score in Subset-A from <strong>51.7 to 56.4</strong>, and in Subset-B from <strong>54.3 to 61.7</strong>. 
                    Further incorporating SDMap alongside camera and LiDAR sensors raises the OLS score in Subset-A to <strong>58.4</strong>. 
                    These results underscore the effectiveness of TopoBDA in road topology comprehension and highlight the substantial 
                    benefits of multi-modal fusion.
                </p>
                <p>
                    Additionally, TopoBDA achieves superior results on the OpenLane-V1 benchmark for 3D lane detection, with 
                    F1-scores of <strong>63.9</strong> at a 1.5m distance and <strong>57.9</strong> at a 0.5m distance. 
                    This work contributes toward closing existing gaps in HDMap element prediction, offering a unified framework 
                    for road topology understanding and 3D lane detection in autonomous driving.
                </p>
            </div>
        </div>
    </section>

    <!-- Citation Section -->
    <section class="section" id="citation">
        <div class="container">
            <h2>Citation</h2>
            <div class="citation-box">
                <pre id="citation-text">@article{kalfaoglu2024topobda,
  title={TopoBDA: Towards Bezier Deformable Attention for Road Topology Understanding},
  author={Kalfaoglu, Muhammet Esat and Ozturk, Halil Ibrahim and Kilinc, Ozsel and Temizel, Alptekin},
  journal={arXiv preprint arXiv:2412.18951},
  year={2024}
}</pre>
                <button class="copy-btn" onclick="copyToClipboard()">üìã Copy Citation</button>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2024 TopoBDA Research. All rights reserved.</p>
            <p>üìß Contact: <a href="mailto:esat.kalfaoglu@metu.edu.tr">esat.kalfaoglu@metu.edu.tr</a></p>
        </div>
    </footer>

    <!-- Image Modal -->
    <div id="imageModal" class="image-modal">
        <span class="close">&times;</span>
        <img id="modalImage" src="" alt="">
    </div>

    <script src="script.js"></script>
</body>
</html>
